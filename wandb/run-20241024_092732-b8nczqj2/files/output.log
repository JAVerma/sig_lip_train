Epoch 1/100:   0%|                                                                                                                        | 0/1500 [00:02<?, ?it/s]
Traceback (most recent call last):
  File "/home/jayant/sig_lip_train/train_with_head.py", line 207, in <module>
    scaler.unscale_(optimizer)  # Unscale gradients
  File "/home/jayant/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 338, in unscale_
    optimizer_state["found_inf_per_device"] = self._unscale_grads_(
  File "/home/jayant/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 260, in _unscale_grads_
    raise ValueError("Attempting to unscale FP16 gradients.")
ValueError: Attempting to unscale FP16 gradients.
