Epoch 1/100:   0%|                                                                                                                        | 0/1500 [00:00<?, ?it/s]/home/jayant/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):
Epoch 1/100:   1%|â–‹                                                                                                              | 10/1500 [00:17<43:17,  1.74s/it]
Traceback (most recent call last):
  File "/home/jayant/sig_lip_train/train_with_head_distributed.py", line 209, in <module>
    loss.backward()
  File "/home/jayant/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/jayant/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/jayant/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/autograd/graph.py", line 768, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
